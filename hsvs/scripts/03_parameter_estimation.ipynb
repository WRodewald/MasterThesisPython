{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#hsvs includes\n",
    "from hsvs.tools import dataset, framed_audio, magnitude, synthesis\n",
    "from hsvs.model import TemporalVarianceLayer, ParameterLayer, BSplineLayer, ZPKToMagLayer, LFRdLayer, util\n",
    "import hsvs\n",
    "\n",
    "# 3rd party dependencies\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io as sio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import soundfile\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vocalset = True # set to true if vocalset directory is not set. If True, uses data/input source samples\n",
    "vowel  = 'a'  # [a,e,i,o,u] or [a,i,o] if no_vocalset == True\n",
    "singer = 'f4' # [f1 - f9, m1-m11] or [f1, f4, f5, f6, f8, m1, m2, m4, m5, m8] if no_vocalset == True\n",
    "\n",
    "# collecting sorce / output file paths \n",
    "singer_vowel_dir = singer + '_' + vowel\n",
    "data_path   = os.path.abspath(os.path.join(os.path.dirname(hsvs.__file__), os.pardir, 'data'))\n",
    "\n",
    "if(no_vocalset):\n",
    "    source_file = os.path.join(data_path, 'input', singer + '_scales_c_slow_forte_' + vowel + '.wav' )\n",
    "else:\n",
    "    source_file = dataset.get_sample('scales', 'slow_forte', vowel, singer)[0]\n",
    "\n",
    "json_file   = os.path.join(data_path, 'results', singer_vowel_dir, 'audio.json' )\n",
    "mat_file    = os.path.join(data_path, 'results', singer_vowel_dir, 'parameters.mat' )\n",
    "wav_file    = os.path.join(data_path, 'results', singer_vowel_dir, 'audio', 'synthesis_estimated.wav' )\n",
    "\n",
    "# loading stored audio object\n",
    "audio = framed_audio.FramedAudio.from_json(json_file, wav_replacement=source_file)\n",
    "\n",
    "# run onset / offset analysis, add some margin to the values\n",
    "onset, offset = magnitude.get_onset_offset(audio)\n",
    "onset  += 50\n",
    "offset -= 50\n",
    "\n",
    "# prepare vectors for input (pitch) and output (overtone decibel magnitude)\n",
    "overtones = audio.get_trajectory('overtones')[onset:offset, :]\n",
    "pitch     = np.expand_dims(audio.get_trajectory('pitch')[onset:offset],1)\n",
    "\n",
    "num_overtones = overtones.shape[1]\n",
    "num_samples = pitch.shape[0]\n",
    "sample_rate = audio.fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow Keras Model**\n",
    "\n",
    "The tensorflow keras model consists of three branches that calculate the decibel response of gain, glottal flow (LF Rd model) and vocal tract (pole/zero). These are combined additively to calculate the models overtone amplitudes in decibel. Parameter trajectory are modeled as 3rd order B-Spline with varying number of B-Spline parameters per frame. Temporal variance loss is introduced in addition to the decibel magnitude loss (weighted mse) to prevent temporal overfitting of parameter trajectories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network to be used to parameter estimation\n",
    "\n",
    "# vocal tract filter parameters \n",
    "num_poles_zeros = 10  \n",
    "num_vt_parameters  = 4 * num_poles_zeros          \n",
    "\n",
    "# number of bspline samples to model parameter Trajectories       \n",
    "pz_b_spline_size   = math.ceil(num_samples / 10) \n",
    "Rd_b_spline_size   = math.ceil(num_samples / 10)\n",
    "gain_b_spline_size = math.ceil(num_samples / 10)\n",
    "\n",
    "# clear seesion to prevent clutter from previously compiled models\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# dummy input used for ZPK layer to have access to pitch\n",
    "inputs = tf.keras.Input(shape=(1,), name='input')\n",
    "\n",
    "# pole/zero (pz) branch\n",
    "var_pz = TemporalVarianceLayer.TemporalVarianceLayer(num_samples = pz_b_spline_size, weight=10E8) \n",
    "x_pz = ParameterLayer.ParameterLayer(pz_b_spline_size, num_vt_parameters, initial_value=0.)(inputs) \n",
    "x_pz = var_pz(x_pz)\n",
    "x_pz = BSplineLayer.BSplineLayer(pz_b_spline_size, num_samples)(x_pz)\n",
    "\n",
    "x_pz, _, _, _ = ZPKToMagLayer.ZPKToMagLayer(sample_rate, num_overtones, name='PZ')([x_pz, inputs])\n",
    "x_pz = util.MagToDBLayer()(x_pz)\n",
    "\n",
    "# Rd branch\n",
    "var_Rd = TemporalVarianceLayer.TemporalVarianceLayer(num_samples = Rd_b_spline_size, weight=10E8) \n",
    "x_Rd = ParameterLayer.ParameterLayer(Rd_b_spline_size, 1, initial_value=0.7)(inputs)\n",
    "x_Rd = var_Rd(x_Rd)\n",
    "x_Rd = BSplineLayer.BSplineLayer(Rd_b_spline_size, num_samples)(x_Rd)\n",
    "x_Rd = tf.keras.layers.Lambda(lambda x: util.lin_scale(tf.sigmoid(x), 0., 1., 0.3, 2.7), name=\"Rd\")(x_Rd)\n",
    "x_Rd = LFRdLayer.LFRdLayer(num_overtones, name=\"RdOut\")(x_Rd)\n",
    "x_Rd = util.MagToDBLayer()(x_Rd)\n",
    "\n",
    "# gain branch\n",
    "var_gain = TemporalVarianceLayer.TemporalVarianceLayer(num_samples = gain_b_spline_size, weight=10E8) \n",
    "x_gain = ParameterLayer.ParameterLayer(gain_b_spline_size, 1, initial_value=0.5)(inputs)\n",
    "x_gain = var_gain(x_gain)\n",
    "x_gain = BSplineLayer.BSplineLayer(gain_b_spline_size, num_samples)(x_gain)\n",
    "x_gain = tf.keras.layers.Lambda(lambda x: util.lin_scale(x, 0., 1., -100, 0), name=\"gain\")(x_gain) # gain scaling\n",
    "x_gain = tf.keras.layers.Lambda(lambda x: tf.tile(x, [1, num_overtones]))(x_gain) # tiling to fit dimensions\n",
    "\n",
    "# gain layer to \"hide\" Rd branch ininitially to improve fitting\n",
    "Rd_gain = util.GainLayer()\n",
    "x_Rd = Rd_gain(x_Rd)\n",
    "\n",
    "# model prediction H = H_vt * H_gf * gain or equivalent addition in decibel domain\n",
    "x = tf.keras.layers.Add()([x_pz, x_gain, x_Rd])\n",
    "\n",
    "# define network model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[x])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Compilation**\n",
    "\n",
    "In addition to the temporal variance loss, the main loss is calculated from the mean squared error of weighted overtone amplitudes in decibel. For weighting, a -6dB / octave low pass filter (cutoff = 2kHz) is used to.  \n",
    "Adam ist used as an optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variance loss weights for pole/zero, gain and Rd parameters\n",
    "var_pz.weight  = 10E9\n",
    "var_gain.weight = 10E0\n",
    "var_Rd.weight   = 10E0\n",
    "\n",
    "# set Rd mix\n",
    "Rd_gain.gain.assign(1.)\n",
    "\n",
    "# constructing a weight matrix with a -6dB/oct lowpass\n",
    "fc = 1000.\n",
    "fk = pitch * np.arange(1, 41) \n",
    "weights  = tf.cast(tf.abs(1. / (1. + 1j * fk/fc)), dtype='float32')\n",
    "weights  = weights\n",
    "weights /= tf.reduce_mean(tf.reduce_mean(weights, axis=1))\n",
    "\n",
    "#compile model with learning rate and loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10E-4),\n",
    "    loss= lambda y_true, y_pred: util.weighted_mse_loss(y_true, y_pred, weights))\n",
    "\n",
    "#print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "The first few epoches are trained without the impact of LF-Rd, basically reducing the model to an source-filter with flat excitation. This is done to prevent overfitting certain frequency ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 16000\n",
    "\n",
    "# custom tqdm progress par to reduce logging noise\n",
    "pbar = tqdm(total=num_epochs)\n",
    "def tqdm_update(epoch, logs):\n",
    "    pbar.update()\n",
    "    pbar.set_postfix_str(\"Loss: %s\" % logs['loss'])\n",
    "\n",
    "model.fit(x=pitch, y=overtones, \n",
    "          shuffle=False,\n",
    "          epochs=num_epochs, \n",
    "          batch_size=num_samples,\n",
    "          callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=tqdm_update)],\n",
    "          verbose=0)\n",
    "pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training w/ Rd**\n",
    "\n",
    "The following cell runs the trianing while gradually adding the LF-Rd decibel offset to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# update allowed filter variance\n",
    "var_pz.weight  = 10E7\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10E-4),\n",
    "    loss= lambda y_true, y_pred: util.weighted_mse_loss(y_true, y_pred, weights))\n",
    "\n",
    "num_epochs = 8000\n",
    "\n",
    "# custom tqdm progress par to reduce logging noise\n",
    "pbar = tqdm(total=num_epochs)\n",
    "def tqdm_update(epoch, logs):\n",
    "    pbar.update()\n",
    "    pbar.set_postfix_str(\"Loss: %s\" % logs['loss'])\n",
    "\n",
    "model.fit(x=pitch, y=overtones, \n",
    "        shuffle=False,\n",
    "        epochs=num_epochs, \n",
    "        batch_size=num_samples,\n",
    "        callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=tqdm_update)],\n",
    "        verbose=0)\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract parameter trajectories from model\n",
    "_, z0_out, p0_out, _ = model.get_layer('PZ').output\n",
    "RdOut_out = model.get_layer('RdOut').output\n",
    "Rd_out    = model.get_layer('Rd').output\n",
    "gain_out  = model.get_layer('gain').output\n",
    "zpk_model = tf.keras.models.Model(inputs=model.input, outputs=[z0_out, p0_out, Rd_out, RdOut_out, gain_out])\n",
    "\n",
    "[z0_pred, p0_pred, Rd_pred, RdOut_pred, gain_pred] = zpk_model.predict(pitch, batch_size=num_samples)\n",
    "\n",
    "# store as .mat file\n",
    "sio.savemat(mat_file, {\n",
    "    'f':pitch,          # pitch\n",
    "    'g':gain_pred,          # gain\n",
    "    'Rd':Rd_pred,           # Rd parameter\n",
    "    'z0':z0_pred,           # filter zeros\n",
    "    'p0': p0_pred,           # filter zeros\n",
    "    'fs': audio.fs,           # filter zeros\n",
    "    'ot': overtones})          # filter poles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = synthesis.run(pitch, Rd_pred, gain_pred, p0_pred, z0_pred, fs=audio.fs, hop_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs if they don't exist yet\n",
    "audio_path = os.path.dirname(wav_file)\n",
    "if not os.path.exists(audio_path):\n",
    "            os.makedirs(audio_path)\n",
    "            \n",
    "# store synthesized audio \n",
    "soundfile.write(wav_file, x * (0.5/max(abs(x))), int(audio.fs))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('MT': conda)",
   "metadata": {
    "interpreter": {
     "hash": "44434d2daad41ca3bd3c074deb06713d7e9ad603c2415ae80165678b3585721f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}