{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pitch Estimation**\n",
    "\n",
    "This script performs the pitch estimation in two stages. First, CREPE is used for an intial pitch estimate. After that, on optimization method is used to improve the estimates accuracy and simultaniously extract the pitch derivative over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#hsvs includes\n",
    "from hsvs.tools import dataset, crepe_estimate, framed_audio, pitch_optimize, magnitude\n",
    "import hsvs\n",
    "\n",
    "# 3rd party dependencies\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel  = 'i'  # [a,e,i,o,u]\n",
    "singer = 'f8' # [f1 - f9, m1-m11]\n",
    "\n",
    "frame_config = { \n",
    "        'centered':True,\n",
    "        'block-size':2048,\n",
    "        'hop-size':64 }\n",
    "\n",
    "pitch_estimate_config = {\n",
    "        'iterations': 400,    # number of gradient descent iterations\n",
    "        'batch-size': 4096,     # batch size for paralle prediction\n",
    "        'pitch-rate': 0.1,      # gradient descent rate for pitch\n",
    "        'pitch-inc-rate': 4000, # gradient descent rate for pitch inc\n",
    "        'rate-decay':0.1}       # target rate factor after 'iterations' epoches.\n",
    "\n",
    "# collecting sorce / output file paths \n",
    "source_file = dataset.get_sample('scales', 'slow_forte', vowel, singer)[0]\n",
    "data_path   = os.path.abspath(os.path.join(os.path.dirname(hsvs.__file__), os.pardir, 'data'))\n",
    "json_file   = os.path.join(data_path, 'results', vowel + '_' + singer, 'audio.json' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREPE Estimate**\n",
    "\n",
    "The initial pitch estimate is provided by CREPE and is later resampled to the required frame rate.\n",
    "\n",
    "CREPE: A Convolutional Representation for Pitch Estimation  \n",
    "Jong Wook Kim, Justin Salamon, Peter Li, Juan Pablo Bello.  \n",
    "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading audio\n",
    "audio = framed_audio.FramedAudio.from_file(source_file,  config=frame_config)\n",
    "\n",
    "# running crepe estimation w/ octave error correciton\n",
    "pitch_crepe = crepe_estimate.predict(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "In the next step, the optimized pitch and pitch increment estimate is extracted using the initial estimate provided by CREPE. \n",
    "Pitch_optimize_gpu uses tensorflow to perform this task parallelized, optionally on a GPU. This step might take a while, a few minutes on a Nvidia RTX 2080 Super "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running optimization / pitch & pitch increment estimation\n",
    "pitch, pitch_inc = pitch_optimize.pitch_optimize_gpu(audio, pitch_estimate = pitch_crepe, options=pitch_estimate_config)\n",
    "\n",
    "# store results as trajectories in audio object\n",
    "audio.store_trajectory('pitch', pitch)\n",
    "audio.store_trajectory('pitch-inc', pitch_inc)\n",
    "audio.store_trajectory('crepe', pitch_crepe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pitch and pitch increment trajectories \n",
    "onset, offset = magnitude.get_onset_offset(audio)\n",
    "plt.plot(audio.get_time(), audio.get_trajectory('pitch'), audio.get_time(), audio.get_trajectory('crepe'))\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "_,_,_,_ = plt.axis((onset*(audio.hop_size/audio.fs), offset*(audio.hop_size/audio.fs), y1, y2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results\n",
    "audio.save_json(json_file=json_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitcondaenv2conda68a0a19b354d45eb8db982a605d52750",
   "display_name": "Python 3.7.6 64-bit ('condaenv2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}